{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":12101512,"sourceType":"datasetVersion","datasetId":7618552}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===============================================================\n#  KAGGLE SETUP & IMPORTS\n# ===============================================================\n!pip install -q google-generativeai\n\nimport pandas as pd\nimport os\nimport time\nfrom kaggle_secrets import UserSecretsClient # To get our API key securely\n\nimport google.generativeai as genai\n# Import the specific types for safety settings\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\n\n# For our dynamic few-shot selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# --- Load API Key from Kaggle Secrets ---\ntry:\n    user_secrets = UserSecretsClient()\n    # Get the Google API key from the new secret\n    secret_value = user_secrets.get_secret(\"GOOGLE_API_KEY\") \n    genai.configure(api_key=secret_value)\n    print(\"Google API key loaded successfully.\")\n    \n    # Define robust safety settings\n    safety_settings_config = [\n        {\n            \"category\": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n            \"threshold\": HarmBlockThreshold.BLOCK_NONE,\n        },\n        {\n            \"category\": HarmCategory.HARM_CATEGORY_HARASSMENT,\n            \"threshold\": HarmBlockThreshold.BLOCK_NONE,\n        },\n        {\n            \"category\": HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            \"threshold\": HarmBlockThreshold.BLOCK_NONE,\n        },\n        {\n            \"category\": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n            \"threshold\": HarmBlockThreshold.BLOCK_NONE,\n        },\n    ]\n    \n    # Instantiate the Gemini model\n    gemini_model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')\nexcept Exception as e:\n    print(\"Could not load Google API key or configure model. Make sure it's stored as a secret named 'GOOGLE_API_KEY'.\")\n    print(f\"Error: {e}\")\n    # You might want to stop execution if the key isn't found\n    # For a hackathon, we'll let it continue and fail on the API call\n    gemini_model = None\n\n\n# --- Load Data ---\n# Adjust 'competition-folder-name' to the actual folder name in /kaggle/input/\nDATA_PATH = \"/kaggle/input/kenya-clinical-reasoning-challenge20250407/\"\ntrain_df = pd.read_csv(DATA_PATH + 'train.csv')\ntest_df = pd.read_csv(DATA_PATH + 'test.csv')\nsample_submission_df = pd.read_csv(DATA_PATH + 'SampleSubmission.csv')\n\nprint(f\"Train data shape: {train_df.shape}\")\nprint(f\"Test data shape: {test_df.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T07:43:22.988697Z","iopub.execute_input":"2025-06-09T07:43:22.990054Z","iopub.status.idle":"2025-06-09T07:43:27.207250Z","shell.execute_reply.started":"2025-06-09T07:43:22.990021Z","shell.execute_reply":"2025-06-09T07:43:27.206022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n#  DYNAMIC FEW-SHOT SELECTOR (Our \"Secret Sauce\")\n# ===============================================================\n# We will use TF-IDF to find the most similar prompts in the training data\n# to use as examples in our LLM prompts. This is much better than static examples.\n\n# Pre-process the training data for searching\nvectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\ntrain_prompt_vectors = vectorizer.fit_transform(train_df['Prompt'])\n\ndef get_most_similar_examples(query_prompt: str, top_n: int = 3) -> pd.DataFrame:\n    \"\"\"Finds the top_n most similar training examples to a given query prompt.\"\"\"\n    query_vector = vectorizer.transform([query_prompt])\n    similarities = cosine_similarity(query_vector, train_prompt_vectors)\n    # Get the indices of the top_n most similar prompts\n    top_indices = np.argsort(similarities[0])[-top_n:][::-1]\n    return train_df.iloc[top_indices]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T07:43:27.210190Z","iopub.execute_input":"2025-06-09T07:43:27.210629Z","iopub.status.idle":"2025-06-09T07:43:27.258731Z","shell.execute_reply.started":"2025-06-09T07:43:27.210592Z","shell.execute_reply":"2025-06-09T07:43:27.257699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n#  STEP 1: PREDICT SNOMED CODES\n# ===============================================================\n\ndef create_snomed_prompt(prompt_text: str) -> str:\n    \"\"\"Creates a dynamic few-shot prompt for SNOMED prediction.\"\"\"\n    examples_df = get_most_similar_examples(prompt_text, top_n=2)\n    \n    prompt = \"You are a medical coding expert. Your task is to analyze a clinical vignette and extract the most relevant SNOMED CT diagnostic codes in the format 'code | description'.\\n\\n\"\n    prompt += \"--- EXAMPLES ---\\n\"\n    \n    for _, row in examples_df.iterrows():\n        prompt += f\"Vignette: \\\"{row['Prompt']}\\\"\\n\"\n        prompt += f\"SNOMED Codes:\\n{row['DDX SNOMED']}\\n\\n\"\n        \n    prompt += \"--- TASK ---\\n\"\n    prompt += f\"Vignette: \\\"{prompt_text}\\\"\\n\"\n    prompt += \"SNOMED Codes:\\n\"\n    \n    return prompt\n\ndef predict_snomed_codes(prompt_text: str) -> str:\n    \"\"\"Uses a dynamic few-shot prompt to predict SNOMED codes.\"\"\"\n    full_prompt = create_snomed_prompt(prompt_text)\n    \n    try:\n        # Configuration for deterministic output\n        generation_config = genai.GenerationConfig(temperature=0.0, max_output_tokens=65536)\n        \n        response = gemini_model.generate_content(full_prompt, generation_config=generation_config, safety_settings=safety_settings_config)\n        return response.text.strip()\n    except Exception as e:\n        print(f\"API Error in SNOMED prediction: {e}\")\n        # Try to see if there's a block reason\n        try:\n            print(response.prompt_feedback)\n        except:\n            pass\n        return \"Error: Could not predict SNOMED codes.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T07:43:27.259814Z","iopub.execute_input":"2025-06-09T07:43:27.260122Z","iopub.status.idle":"2025-06-09T07:43:27.269459Z","shell.execute_reply.started":"2025-06-09T07:43:27.260099Z","shell.execute_reply":"2025-06-09T07:43:27.268259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n#  STEP 2: GENERATE CLINICIAN RESPONSE\n# ===============================================================\n\ndef create_response_prompt(prompt_text: str, snomed_codes: str) -> str:\n    \"\"\"Creates a dynamic few-shot prompt for the clinician response.\"\"\"\n    examples_df = get_most_similar_examples(prompt_text, top_n=1)\n    \n    prompt = \"You are an expert Kenyan clinician providing practical guidance to a colleague. Your response MUST match the persona of the nurse in the prompt and the resources available at their facility. Structure your response with a summary, diagnosis, and a clear management plan. Emulate the style and reasoning from the examples below. Be concise and direct.\\n\\n\"\n    prompt += \"--- EXAMPLES ---\\n\"\n    \n    for _, row in examples_df.iterrows():\n        prompt += f\"Vignette: \\\"{row['Prompt']}\\\"\\n\"\n        prompt += f\"Key Diagnoses: {row['DDX SNOMED']}\\n\"\n        prompt += f\"Clinician Response: {row['Clinician']}\\n\\n\"\n        \n    prompt += \"--- TASK ---\\n\"\n    prompt += f\"Vignette: \\\"{prompt_text}\\\"\\n\"\n    prompt += f\"Key Diagnoses: {snomed_codes}\\n\"\n    prompt += \"Clinician Response:\\n\"\n    \n    return prompt\n\ndef generate_clinician_response(prompt_text: str, snomed_codes: str) -> str:\n    \"\"\"Uses a dynamic few-shot prompt to generate a clinician's response.\"\"\"\n    full_prompt = create_response_prompt(prompt_text, snomed_codes)\n    \n    try:\n        # Configuration for natural language output\n        generation_config = genai.GenerationConfig(temperature=0.4, max_output_tokens=65536)\n\n        response = gemini_model.generate_content(full_prompt, generation_config=generation_config, safety_settings=safety_settings_config)\n        return response.text.strip()\n    except Exception as e:\n        print(f\"API Error in response generation: {e}\")\n        try:\n            print(response.prompt_feedback)\n        except:\n            pass\n        return \"Error: Could not generate response.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T07:43:27.271569Z","iopub.execute_input":"2025-06-09T07:43:27.272029Z","iopub.status.idle":"2025-06-09T07:43:27.295895Z","shell.execute_reply.started":"2025-06-09T07:43:27.271999Z","shell.execute_reply":"2025-06-09T07:43:27.294677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n#  MAIN EXECUTION SCRIPT\n# ===============================================================\nif gemini_model:\n    results = []\n    print(\"Starting prediction process with Google Gemini...\")\n\n    # --- Rate Limiting Setup ---\n    RPM = 10  # Requests Per Minute\n    requests_this_minute = 0\n    start_time = time.time()\n    \n    for row in test_df.itertuples():\n        print(f\"Processing row {row.Index + 1}/{len(test_df)}: Master_Index = {row.Master_Index}\")\n        \n        prompt_text = row.Prompt\n        \n        # Step 1: Predict SNOMED codes\n        predicted_snomed = predict_snomed_codes(prompt_text)\n        print(f\"  > Predicted SNOMEDs: {repr(predicted_snomed[:70])}...\")\n\n        # Step 2: Generate the final clinician response\n        final_response = generate_clinician_response(prompt_text, predicted_snomed)\n        print(f\"  > Generated Response: {repr(final_response[:70])}...\")\n        \n        results.append({\n            'Master_Index': row.Master_Index,\n            'Clinician': final_response\n        })\n        print(\"-\" * 20)\n\n        # --- Rate Limiting Logic ---\n        requests_this_minute += 2 # Each row uses two API calls\n        if requests_this_minute >= RPM:\n            elapsed_time = time.time() - start_time\n            # if the minute hasn't passed yet\n            if elapsed_time < 60:\n                sleep_time = 60 - elapsed_time\n                print(f\"  > Rate limit reached. Sleeping for {sleep_time:.2f} seconds...\")\n                time.sleep(sleep_time)\n                start_time = time.time()  # Reset the timer\n                requests_this_minute = 0 # Reset the counter\n            else: # A minute has passed\n                start_time = time.time()\n                requests_this_minute = 0\n\n    print(\"Prediction process finished.\")\n    \n    submission_df = pd.DataFrame(results)\n    submission_df = submission_df[sample_submission_df.columns] \n    submission_df.to_csv('submission.csv', index=False)\n    \n    print(\"Submission file 'submission.csv' has been created successfully!\")\n    print(submission_df.head())\nelse:\n    print(\"Execution halted because Gemini model could not be initialized.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T07:43:27.297166Z","iopub.execute_input":"2025-06-09T07:43:27.297560Z","iopub.status.idle":"2025-06-09T07:44:01.506941Z","shell.execute_reply.started":"2025-06-09T07:43:27.297529Z","shell.execute_reply":"2025-06-09T07:44:01.505100Z"}},"outputs":[],"execution_count":null}]}